"use strict";(globalThis.webpackChunkatelier=globalThis.webpackChunkatelier||[]).push([[448],{441:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>i});const l=JSON.parse('{"id":"llama-cpp-python","title":"llama-cpp-python","description":"Notes on using llama-cpp-python.","source":"@site/docs/llama-cpp-python.md","sourceDirName":".","slug":"/llama-cpp-python","permalink":"/perpus/llama-cpp-python","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"Python","permalink":"/perpus/tags/python"},{"inline":true,"label":"LLM","permalink":"/perpus/tags/llm"}],"version":"current","lastUpdatedAt":1755043200000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"llama-cpp-python","tags":["Python","LLM"],"last_update":{"date":"2025-08-13T00:00:00.000Z"}},"sidebar":"docsSidebar","previous":{"title":"perpus","permalink":"/perpus/"},"next":{"title":"SSH to WSL","permalink":"/perpus/development/ssh-to-wsl"}}');var o=t(7259),a=t(9087);const s={sidebar_position:2,sidebar_label:"llama-cpp-python",tags:["Python","LLM"],last_update:{date:new Date("2025-08-13T00:00:00.000Z")}},r="llama-cpp-python",p={},i=[{value:"Basic Usage",id:"basic-usage",level:3},{value:"Enabling Logits on Output Tokens",id:"enabling-logits-on-output-tokens",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"llama-cpp-python",children:"llama-cpp-python"})}),"\n",(0,o.jsxs)(n.p,{children:["Notes on using ",(0,o.jsx)(n.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"llama-cpp-python"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n    repo_id="google/gemma-3-1b-it-qat-q4_0-gguf",\n    filename="*q4_0.gguf",\n    n_gpu_layers=-1,\n    n_ctx=4096,\n    verbose=False,\n    seed=42,\n)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"enabling-logits-on-output-tokens",children:"Enabling Logits on Output Tokens"}),"\n",(0,o.jsxs)(n.p,{children:["To obtain logits for all tokens, set ",(0,o.jsx)(n.code,{children:"logits_all=True"})," when initializing the model.\nThis enables the model to return ",(0,o.jsx)(n.code,{children:"logprobs"})," values in completions instead of ",(0,o.jsx)(n.code,{children:"None"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"llm = Llama.from_pretrained(\n    ...,\n    logits_all=True # this is required\n)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["When generating a completion, set both ",(0,o.jsx)(n.code,{children:"logprobs"})," and ",(0,o.jsx)(n.code,{children:"top_logprobs"})," in the call.\nIf only ",(0,o.jsx)(n.code,{children:"logprobs"})," is set, ",(0,o.jsx)(n.code,{children:"top_logprobs"})," defaults to ",(0,o.jsx)(n.code,{children:"None"}),", and no ",(0,o.jsx)(n.code,{children:"logprobs"})," are returned (",(0,o.jsx)(n.a,{href:"https://github.com/abetlen/llama-cpp-python/blob/main/llama_cpp/llama_chat_format.py#L676",children:"reference"}),")."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"response = llm.create_chat_completion(\n    ...,\n    logprobs=True,\n    top_logprobs=10 # list top 10 probability for each token\n)\n"})})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},9087:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var l=t(6363);const o={},a=l.createContext(o);function s(e){const n=l.useContext(a);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),l.createElement(a.Provider,{value:n},e.children)}}}]);